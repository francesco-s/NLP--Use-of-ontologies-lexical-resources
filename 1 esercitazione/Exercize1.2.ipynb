{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75bac711-9ab9-48ee-bf9a-643ad2682087",
   "metadata": {},
   "source": [
    "## Exercize 1.2\n",
    "### Word Sense Disambiguation with WordNet\n",
    "#### Francesco Sannicola"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a039e20-a64e-4929-a8d4-3a5f0f9faa0c",
   "metadata": {},
   "source": [
    "Import some useful libraries like *nltk* and *string*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "523ae500-e6d9-4cdc-9c45-d6572d8a36a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722c6538-76a0-44d9-8b06-3619d6391d8d",
   "metadata": {},
   "source": [
    "Method which parses SemCor corpus. The corpus is annotated by hand on WordNet.\n",
    "\n",
    "After reading the XML file, it fix XML's bad formatting and obtains all the sentences. I use re and lxml packages for regular expression e exml files.\n",
    "\n",
    "Now selects the words to disambiguate, with plus then one total number of senses, and extract Golden annotated sense from Wordnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9acb7b1-cb6a-4a75-8cba-c287c41aa81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from lxml import etree as Exml\n",
    "\n",
    "def xml_parse(file):\n",
    "    \"\"\"\n",
    "    :param file: the path to the XML file\n",
    "    :return: (sentence, [(word, gold)])\n",
    "    \"\"\"\n",
    "    with open(file, 'r') as XML_file:\n",
    "        data = XML_file.read()\n",
    "        \n",
    "        # Delete all \\n\n",
    "        data = data.replace('\\n', '')\n",
    "        \n",
    "        # Delete all special char\n",
    "        rep = re.compile(\"=([\\w|:|\\-|$|(|)|']*)\")\n",
    "        data = rep.sub(r'=\"\\1\"', data)\n",
    "\n",
    "        res = []\n",
    "        try:\n",
    "            \n",
    "            complete_xml_processed = Exml.XML(data)\n",
    "            \n",
    "            # Obtain all paragraphs\n",
    "            par = complete_xml_processed.findall(\"./context/p\")\n",
    "            sentences = []\n",
    "            \n",
    "            # For each paragraph, take all sentences\n",
    "            for p in par:\n",
    "                sentences.extend(p.findall(\"./s\"))\n",
    "            \n",
    "            # Process all sentences\n",
    "            for sentence in sentences:\n",
    "                \n",
    "                # Take all words in a sentence\n",
    "                words = sentence.findall('wf')\n",
    "                complete_sentence = \"\"\n",
    "                tuples = []\n",
    "                \n",
    "                for word in words:\n",
    "                    \n",
    "                    # With Exml it's easy to find word PoS \n",
    "                    w = word.text\n",
    "                    pos = word.attrib['pos']\n",
    "                    complete_sentence = complete_sentence + w + ' '\n",
    "                    \n",
    "                    # In a new strucure save only nouns whith sense id\n",
    "                    if pos == 'NN' and len(wn.synsets(w)) > 1 and 'wnsn' in word.attrib and '_' not in w :\n",
    "                        sense_id = word.attrib['wnsn']\n",
    "                        tuples.append((w, sense_id))\n",
    "                        \n",
    "                res.append((complete_sentence, tuples))\n",
    "        except Exception as e:\n",
    "            raise NameError(str(e))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8549995e-6d8c-4aff-b953-f7788e3f3e09",
   "metadata": {},
   "source": [
    "Implementation *bag of word* algoritm.\n",
    "\n",
    "Given a sentence compute a result according to the bag of word approach: an unordered set of words, ignoring their exact position.\n",
    "\n",
    "There is a preprocessing phase with the goal to eliminate stop words and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77c00a4b-a486-4fc0-8bf4-2f520b61cbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_word(sentence):\n",
    "    \"\"\"\n",
    "    :param sentence: sentence\n",
    "    :return: bag of words\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    wordnet_lemmatizer = nltk.WordNetLemmatizer()\n",
    "    \n",
    "    # tokenization of a given sentence\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    # delete punctuation and stop words\n",
    "    tokens = list(filter(lambda x: x not in stop_words and x not in string.punctuation, tokens))\n",
    "    \n",
    "    # unsorted list of lemmas\n",
    "    res = set(wordnet_lemmatizer.lemmatize(token) for token in tokens)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33952742-f30b-4ec2-8656-8a41a503a9e9",
   "metadata": {},
   "source": [
    "### Lesk's algoritm implementation. \n",
    "\n",
    "Given a word and a sentence which contains that word, it return the best sense of that word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d87ffe6-e248-4c0b-89af-798abd5af29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lesk(word, sentence):\n",
    "    \"\"\"\n",
    "    :param word: word to disabiguate\n",
    "    :param sentence: sentence to compare\n",
    "    :return: best sense for the given word\n",
    "    \"\"\"\n",
    "\n",
    "    max_overlap = 0\n",
    "    senses = wn.synsets(word)\n",
    "    \n",
    "    # set word's first sense as the best\n",
    "    best_sense = senses[0]\n",
    "    \n",
    "    # compute context of the sentence (BAW)\n",
    "    context = bag_of_word(sentence)\n",
    "\n",
    "    for sense in senses:\n",
    "        \n",
    "        # compute BAW of sense definition\n",
    "        signature = bag_of_word(sense.definition())\n",
    "\n",
    "        # for each example calculate BAW and calculate the union (no duplicates)\n",
    "        examples = sense.examples()\n",
    "        for ex in examples:\n",
    "            signature = signature.union(bag_of_word(ex))\n",
    "\n",
    "        # Calculate overlap between BOW of signature and BAW of context\n",
    "        overlap = len(signature & context)\n",
    "        \n",
    "        # save only the best sense (with more overlap)\n",
    "        if overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "            best_sense = sense\n",
    "\n",
    "    return best_sense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49255213-8af3-4b6e-9a57-ae39ae9ef04b",
   "metadata": {},
   "source": [
    "Method which invokes Lesk algorithm and compares the inferred senses and the right ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9b84ec3-cca4-4134-ad4d-312df73cb721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_sense_disambiguation(xml_file_parsed, limit):\n",
    "    '''\n",
    "    :param sentence: input sentence\n",
    "    :param limit: maximum number of sentences to disambiguate\n",
    "    :return: right senses, number of terms analyzed  \n",
    "    '''\n",
    "    right = 0\n",
    "    count = 0\n",
    "    terms_analyzed = 0 \n",
    "\n",
    "    for instance in xml_file_parsed:\n",
    "        if count == limit:\n",
    "            break\n",
    "        \n",
    "        if len(instance[1]) > 0:\n",
    "            # NB instance is a tuple ('sentence', [('word','number`]...])\n",
    "            # calculate best sense of the first noun using lesk\n",
    "            my_sense = lesk(instance[1][0][0], instance[0])\n",
    "            \n",
    "            # Index of the sense calculated with lesk algorithm considering entire synset\n",
    "            index_val = wn.synsets(instance[1][0][0]).index(my_sense) + 1\n",
    "\n",
    "            # obtain gold\n",
    "            annotated_index = int(instance[1][0][1])\n",
    "        \n",
    "            if index_val == annotated_index:\n",
    "                right += 1\n",
    "                \n",
    "            terms_analyzed += 1\n",
    "        count += 1\n",
    "        \n",
    "    if terms_analyzed == 0:\n",
    "        terms_analyzed = 1\n",
    "    \n",
    "    return right, terms_analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9046655-30a4-4857-8c94-97872df83b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------+-------+\n",
      "| Iteration | Accuracy | Right | Total |\n",
      "+-----------+----------+-------+-------+\n",
      "|     0     |  0.5417  |   26  |   48  |\n",
      "|     1     |   0.6    |   30  |   50  |\n",
      "|     2     |  0.6122  |   30  |   49  |\n",
      "|     3     |  0.6327  |   31  |   49  |\n",
      "|     4     |  0.6122  |   30  |   49  |\n",
      "|     5     |  0.5918  |   29  |   49  |\n",
      "|     6     |   0.56   |   28  |   50  |\n",
      "|     7     |  0.617   |   29  |   47  |\n",
      "|     8     |  0.6383  |   30  |   47  |\n",
      "|     9     |  0.5833  |   28  |   48  |\n",
      "+-----------+----------+-------+-------+\n",
      "Accuracy mean: 0.5989298740772905\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "iterations = 10\n",
    "limit = 50\n",
    "percentuals_sum = 0\n",
    "\n",
    "xml_file_parsed = xml_parse(\"./Input/br-a01\")\n",
    "table = PrettyTable([\"Iteration\", \"Accuracy\", \"Right\", \"Total\"])\n",
    "\n",
    "for i in range (0, iterations):\n",
    "    \n",
    "    right, total = word_sense_disambiguation(xml_file_parsed, limit)\n",
    "    acc = right/total\n",
    "    \n",
    "    table.add_row([i, round(acc, 4), right, total])\n",
    "    \n",
    "    percentuals_sum += acc\n",
    "    \n",
    "    # Shuffle all file's instances (NB a element is (sentence, [(word, gold)]))\n",
    "    random.shuffle(xml_file_parsed)\n",
    "                   \n",
    "print(table)\n",
    "\n",
    "print(\"Accuracy mean:\", percentuals_sum/iterations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e49fa3-c107-4db2-8ec9-9ec90a059ee0",
   "metadata": {},
   "source": [
    "Accuracy obtained is about 59 %. \n",
    "\n",
    "This result is not bad at all considering the difficulties of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e6e26b-f3e0-4e67-a59b-2f706f507746",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
