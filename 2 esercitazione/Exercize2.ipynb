{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afa867d0-aa0a-45f3-bc3a-b1067687479c",
   "metadata": {},
   "source": [
    "## Exercize 2\n",
    "### Mapping Framenet-WordNet\n",
    "#### Francesco Sannicola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd15980f-cee0-4cec-af67-a8d19e88e791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import framenet as fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e87668e-df2c-4471-b73a-54db7cd3de18",
   "metadata": {},
   "source": [
    "---\n",
    "### getFrameSetForStudent\n",
    "\n",
    "Funzione per assegnare a ciascuno un insieme di frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84014644-6a7d-46d3-91b5-21e58afb8836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import random\n",
    "from random import randint\n",
    "from random import seed\n",
    "\n",
    "def print_frames_with_IDs():\n",
    "    for x in fn.frames():\n",
    "        print('{}\\t{}'.format(x.ID, x.name))\n",
    "\n",
    "def get_frams_IDs():\n",
    "    return [f.ID for f in fn.frames()]   \n",
    "\n",
    "def getFrameSetForStudent(surname, list_len=5):\n",
    "    nof_frames = len(fn.frames())\n",
    "    base_idx = (abs(int(hashlib.sha512(surname.encode('utf-8')).hexdigest(), 16)) % nof_frames)\n",
    "    print('\\nstudent: ' + surname)\n",
    "    framenet_IDs = get_frams_IDs()\n",
    "    i = 0\n",
    "    offset = 0 \n",
    "    seed(1)\n",
    "    id_list = []\n",
    "    while i < list_len:\n",
    "        fID = framenet_IDs[(base_idx+offset)%nof_frames]\n",
    "        f = fn.frame(fID)\n",
    "        fNAME = f.name\n",
    "        print('\\tID: {a:4d}\\tframe: {framename}'.format(a=fID, framename=fNAME))\n",
    "        id_list.append(fID)\n",
    "        offset = randint(0, nof_frames)\n",
    "        i += 1        \n",
    "    return id_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d810163-a16c-4951-9233-1bbccd7c8342",
   "metadata": {},
   "source": [
    "Get frame's IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc77810e-c535-4685-be8c-84b4662feb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "student: sannicola\n",
      "\tID: 1597\tframe: Familiarity\n",
      "\tID: 1017\tframe: Noise_makers\n",
      "\tID:   58\tframe: Emptying\n",
      "\tID:  386\tframe: Ingest_substance\n",
      "\tID: 2838\tframe: Scheduling\n"
     ]
    }
   ],
   "source": [
    "frame_IDs = getFrameSetForStudent('sannicola') # + getFrameSetForStudent('Francesco')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7379b5-a3e5-4006-9af5-73e4b6452273",
   "metadata": {},
   "source": [
    "Get frame by frame id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7fe37b5-956d-42e9-8f4c-70d84820ad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame(frame_id):\n",
    "    return fn.frame_by_id(frame_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f318c0b0-cd03-4597-a7bb-084d8174c3f3",
   "metadata": {},
   "source": [
    "Get synset by word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7aa6d652-00ac-46c5-947e-b13861f01647",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def get_synsets(word):\n",
    "    synsets = wn.synsets(word)\n",
    "    # if there isn't a synset for the word it returns \"Null\"\n",
    "    if synsets == \"\":\n",
    "        return \"Null\"\n",
    "    else:\n",
    "        return synsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584157f3-890e-4d1b-a62e-84f03224c702",
   "metadata": {},
   "source": [
    "Use spacy for additional features for our terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbc0df36-080f-420b-9c5e-8059b800d14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "sp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a23dc6-ba26-4559-bb76-78fd0b24f47e",
   "metadata": {},
   "source": [
    "Get of the main word from a composed frame name (it contains \"-\" or \"_\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a99ae516-9dda-41c9-b93c-77a1ca5d89bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_main_word(frame_name):\n",
    "    \"\"\"\n",
    "    :param frame_name: frame name\n",
    "    :return: the main word inside the frame name\n",
    "    \"\"\"\n",
    "    if '-' or '_' in frame_name:\n",
    "        \n",
    "        main_word = \"\"\n",
    "        \n",
    "        # remove some punctualizations\n",
    "        frame_name = frame_name.replace('_', ' ').replace('-', ' ')\n",
    "\n",
    "        # Convert string into iterable obj with additional features (easy PoS tagging), using spacy\n",
    "        frame_name = sp(frame_name)\n",
    "        \n",
    "        for term in frame_name:\n",
    "            # Main word is the NN or NNS in a sentence\n",
    "            if term.tag_ == \"NNS\" or term.tag_ == \"NN\":\n",
    "                main_word = term.text\n",
    "            else:\n",
    "                # Select the \"root\" term\n",
    "                if term.dep_ == 'ROOT':\n",
    "                    main_word = term.text\n",
    "        return main_word\n",
    "    else:\n",
    "        return frame_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86ca5d5-dbcc-4d75-aed0-41ee296d9e03",
   "metadata": {},
   "source": [
    "Some string processing. It creates a list with name + definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "727185cc-473a-40c2-8a85-e72e45a7949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(name, definition):\n",
    "    '''\n",
    "    :param name: FE or LU name\n",
    "    :param definiton: FE or LU definition\n",
    "    :return: processed list (name + definition)\n",
    "    '''\n",
    "    res = []\n",
    "    \n",
    "    # string to delete\n",
    "    other_str_delete = [\"fe\", \"fn\", \"cod\", \"'\", \"$\"]\n",
    "    \n",
    "    # set string to lowercase\n",
    "    definition = definition.lower()\n",
    "    \n",
    "    # transform string into Doc with spacy\n",
    "    definition = sp(definition)\n",
    "\n",
    "    # Now append the name\n",
    "    res.append(name.lower())\n",
    "    for term in definition:\n",
    "        # Save only no stop and no punctalization term\n",
    "        if not term.is_stop:\n",
    "            if term.pos_ != \"PUNCT\" and term.pos_ != \"NUM\":\n",
    "                res.append(term.lemma_)\n",
    "\n",
    "    # In the end delete strings into other_str_delete variable\n",
    "    res = list(filter(lambda x: x not in other_str_delete, res))\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d7ec41-31bf-4133-8b4a-85401d99e3da",
   "metadata": {},
   "source": [
    "Get frame name, frame elements and lexical units for each frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9158f08d-a0dc-4523-b431-51258f6622b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "fn_list = []\n",
    "fe_list = []\n",
    "lu_list = []\n",
    "\n",
    "for id in frame_IDs:\n",
    "    #print (\"-----------------Frame name-----------------\")\n",
    "    f = get_frame(id)\n",
    "    fn_list.append(process(get_main_word(f.name), f.definition))\n",
    "    #print (\"-----------------Frame elements-----------------\")\n",
    "    for key in f.FE:\n",
    "        definition = f.FE[key].definition\n",
    "        main_word = get_main_word(key)\n",
    "        fe_list.append(process(main_word, definition))\n",
    "    #print (\"-----------------Lexical units-----------------\")\n",
    "    for key in f.lexUnit:\n",
    "        lu_key = re.sub('\\.[a-z]+', '', key)\n",
    "        main_word = get_main_word(lu_key)\n",
    "        definition = f.lexUnit[key].definition\n",
    "        lu_list.append(process(main_word, definition))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc4a2f1-147b-4a02-855b-4b2fc44cef77",
   "metadata": {},
   "source": [
    "Given a synset, obtain a list of processed examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1855ff6-d383-4b10-b54f-73ab24b7eeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_examples(synset):\n",
    "    examples = []\n",
    "    if synset.examples():\n",
    "        for example in synset.examples():\n",
    "            examples.append(process(\"\", example))\n",
    "    \n",
    "    # Compose the result\n",
    "    res = [item for sub_list in examples for item in sub_list]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d7714e-3e19-4639-beac-ebd1537198d2",
   "metadata": {},
   "source": [
    "Calculate the context of a synset depending of his hyponyms and hypernyms (with limit = 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90b114ba-380a-4c52-aa4f-c74dcb758c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(synset):\n",
    "    '''\n",
    "    :param synset: WN synset\n",
    "    :return: synset's context\n",
    "    '''\n",
    "    \n",
    "    synset_def = []\n",
    "    current_examples = []\n",
    "    \n",
    "    # take the synset definition and process it\n",
    "    definition = synset.definition()\n",
    "    synset_def = process(\"\", definition)\n",
    "\n",
    "    # get examples of that synset\n",
    "    current_examples = get_examples(synset)\n",
    "\n",
    "    #check hyponyms\n",
    "    hyponyms = synset.hyponyms()\n",
    "    hypon_list = []\n",
    "    hy_def = []\n",
    "    \n",
    "    if hyponyms !=0:\n",
    "        \n",
    "        limit = 0\n",
    "        for hypon in hyponyms:\n",
    "            if limit == 3:\n",
    "                break\n",
    "                \n",
    "            # save hyponym definition and process it\n",
    "            hy_def.append(process(\"\", hypon.definition()))\n",
    "            \n",
    "            # save hyponym examples\n",
    "            hypon_list.append(get_examples(hypon))\n",
    "\n",
    "            limit += 1\n",
    "            \n",
    "    #check hypernyms\n",
    "    hypernyms = synset.hypernyms()\n",
    "    hyper_list = []\n",
    "    \n",
    "    if hypernyms != 0:\n",
    "        limit = 0\n",
    "        for hyper in hypernyms:\n",
    "            if limit == 3:\n",
    "                break\n",
    "                \n",
    "            # save hypernym definition and process it\n",
    "            hy_def.append(process(\"\", hyper.definition()))\n",
    "            \n",
    "            # save hypernym examples\n",
    "            hyper_list.append(get_examples(hyper))\n",
    "            \n",
    "            limit += 1\n",
    "\n",
    "    # Compose hypernym and hyponym definitions and examples\n",
    "    hy_def = [item for sub_list in hy_def for item in sub_list]\n",
    "    hyper_list = [item for sub_list in hyper_list for item in sub_list]\n",
    "    hypon_list = [item for sub_list in hypon_list for item in sub_list]\n",
    "\n",
    "\n",
    "    # Context composed by examples and definiton of current synset + hypernym and hyponym definitions + hypernym and hyponym definitions\n",
    "    return current_examples + synset_def + hy_def + hypon_list + hyper_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b82c401-d849-4efc-94dd-2d043900038c",
   "metadata": {},
   "source": [
    "Read a CSV file and return a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cd5a858-e9e7-4c00-bdf5-4c745281ed27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "\n",
    "def read_csv(path):\n",
    "    '''\n",
    "    :param path: reference to a CSV file.\n",
    "    :return: a list with all file's row.\n",
    "    '''\n",
    "    with open(path, 'r', encoding='utf-8-sig') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter='\\t')\n",
    "        res = []\n",
    "        for row in csv_reader:\n",
    "            res.append(row)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23965e3-cd1f-425a-8261-6798a067136f",
   "metadata": {},
   "source": [
    "Custom implementation of the \"Bag of Word\" algorithm.\n",
    "It calculates best sense given a word and a WN synsets for that word based on context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3001a5f1-29f0-4aa5-b1ac-3b7490cc8ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BOW_best_sense(word, synsets):\n",
    "    '''\n",
    "    :param word: word to map\n",
    "    :param synsets: synsets of that word\n",
    "    :return: best synset\n",
    "    '''\n",
    "    max_score = 0\n",
    "    bag = list(word)\n",
    "    best_synset = None\n",
    "    \n",
    "    if len(synsets) > 1:\n",
    "        for synset in synsets:\n",
    "            # Get context of the synset\n",
    "            context = get_context(synset)\n",
    "            context = list(context)\n",
    "\n",
    "            # Calculate the intersection between word and context (+1 for Smoothing)\n",
    "            score = len(set(word).intersection(context)) + 1\n",
    "                        \n",
    "            # Save the synset with higher score\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                best_synset = synset\n",
    "    else:\n",
    "        best_synset = synsets\n",
    "\n",
    "    return best_synset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cc1b69-b87a-4766-bc50-0159fe827bc1",
   "metadata": {},
   "source": [
    "Reads the hand written golds and compute bag of word between user's annotation and frame's name, frame's elements and lexical units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dff7b9ad-3bfb-4225-aac3-edd68a1997c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_gold_path = 'input/fn_hand.tsv'\n",
    "fe_gold_path = 'input/fe_hand.tsv'\n",
    "lu_gold_path = 'input/lu_hand.tsv'\n",
    "\n",
    "fn_gold = read_csv(fn_gold_path)\n",
    "fe_gold = read_csv(fe_gold_path)\n",
    "lu_gold = read_csv(lu_gold_path)\n",
    "\n",
    "# List with all computed senses\n",
    "bow_res = []\n",
    "\n",
    "for i in range(0, len(fn_gold)):\n",
    "    synsets = wn.synsets(fn_gold[i][0])\n",
    "    best_synset = BOW_best_sense(fn_list[i], synsets)\n",
    "    bow_res.append(best_synset)\n",
    "\n",
    "for i in range(0, len(fe_gold)):\n",
    "    synsets = wn.synsets(fe_gold[i][0])\n",
    "    best_synset = BOW_best_sense(fe_list[i], synsets)\n",
    "    bow_res.append(best_synset)\n",
    "\n",
    "for i in range(0, len(lu_gold)):\n",
    "    synsets = wn.synsets(lu_gold[i][0])\n",
    "    best_synset = BOW_best_sense(lu_list[i], synsets)\n",
    "    bow_res.append(best_synset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7200b9b5-da69-4489-b685-43af3888fbf9",
   "metadata": {},
   "source": [
    "Last block calculates system accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f07de8fb-3a36-4795-acff-68406f1f71dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Right mappings:  67 \n",
      "Wrong mappings:  68\n",
      "Accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "all_gold = fn_gold + fe_gold + lu_gold\n",
    "right = 0\n",
    "\n",
    "for i in range(0, len(bow_res)):\n",
    "    current_mapping = str(bow_res[i]).replace('[', '').replace(']', '').replace(\"Synset('\", '').replace(\"')\", '')\n",
    "    \n",
    "    # Comparison between computed senses and hand written senses\n",
    "    if current_mapping == all_gold[i][1]:\n",
    "        right += 1\n",
    "        \n",
    "print(\"Right mappings: \", right, \"\\nWrong mappings: \", len(bow_res)-right)\n",
    "print(\"Accuracy: \", round(right/len(bow_res), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baadf2ea-03a3-4614-81e7-3b08eed05a76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
